{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text generation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI5M9iQLZMSh",
        "colab_type": "text"
      },
      "source": [
        "# Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nmvQ62JZMSo",
        "colab_type": "text"
      },
      "source": [
        "<center><h3>Spooky Author Identification-Kaggle</h3></center>\n",
        " <center><h3> https://www.kaggle.com/c/spooky-author-identification</h3></center>\n",
        "\n",
        "Here, I try to generate new sentences after training an RNN on around 20,000 sentences taken from the Kaggle dataset above. While the dataset was originally meant for a different purpose, I couldn't find a dedicated dataset for text generation, so I decided to use this one.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YakDHqz-W2x6",
        "colab_type": "text"
      },
      "source": [
        "## Idea\n",
        "\n",
        "I will be using language modeling to generate text. In its essence, language modeling tries to predict the next word, given prior words.  For example, given the word ''Ryan loves cats\" , language model tries to find P((next word)| 'Ryan') and then P((next word)|'Ryan', 'loves'). Ideally, the model must learn that next word is 'loves' and 'cats' for the two cases\n",
        "mentioned above.\n",
        "\n",
        "In the first part, I develop a basic word based rnn model from scratch, that works on the idea mentioned above. This model serves the show the various preprocessing steps required to feed the data and get predictions, it does not have a very good performance. In the final step, I take a pretrained char - level rnn, train it for a few epochs and use that to generate much better predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4XR9foOZMSs",
        "colab_type": "text"
      },
      "source": [
        "## Steps\n",
        "\n",
        "1. First I will preprocess the input text by tokenizing the text, generating a vocabulary of unique words from it, and saving that vocabulary in the file 'vocab.txt'.\n",
        "2. Then I will add three extra tokens - START denoting the start of a sentence, UNK for unknown words and PAD for pad tokens.\n",
        "3. Since I have limited computational power, I will restrict myself to generating sentences of maximum 20 words. This decision was made after observing the frequency of different sentence lengths in the database. All sentences greater than 20 words will be truncated, and shorter sentences will have PAD  tokens till they reach the length 20.\n",
        "4. Before feeding into the RNN, the words will be numerized by using a mapping of words in vocab.txt to corresponding numbers.\n",
        "5. For text generation, we first prepend the START token to each sentence, shifting it to the right by 1 sentence. That will be used as input to predict the output sentence. For example - For the sentence ' (START) Do it', '(START)' will be used to predict 'Do', '(START), Do' will be used to predict 'it' and so on. \n",
        "6. The data will be fed to the network in the form of batches, and LSTM Cell will be used to learn the next word by minimising the cross-entropy softmax function.\n",
        "7. For generating new text, I will enter the first few words. Then the trained model will keep on picking the next most likely word ( given by the argmax) and generate the sentence.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIaJHaETZQ-R",
        "colab_type": "code",
        "outputId": "87affa8e-1d68-4fc5-9be7-da10fd313f14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "!pip install segtok # needed for tokenization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting segtok\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/59/6ed78856ab99d2da04084b59e7da797972baa0efecb71546b16d48e49d9b/segtok-1.5.7.tar.gz\n",
            "Collecting regex (from segtok)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz (651kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 7.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: segtok, regex\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/ee/a8/6112173f1386d33eebedb3f73429cfa41a4c3084556bcee254\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n",
            "Successfully built segtok regex\n",
            "Installing collected packages: regex, segtok\n",
            "Successfully installed regex-2019.6.8 segtok-1.5.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGwXB10JZX0u",
        "colab_type": "code",
        "outputId": "78351d48-3187-4400-b554-11ef4339f381",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg0AM_nfZMS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from segtok import tokenizer\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "import os, sys\n",
        "\n",
        "sys.path.insert(0, \"/content/gdrive/My Drive/text_generation/\") \n",
        "\n",
        "root_folder = \"/content/gdrive/My Drive/text_generation/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci9YRpOhZMT4",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeUUGg3OZMT8",
        "colab_type": "text"
      },
      "source": [
        "The function 'unique_file' opens the input file ('train.txt'), reads it and saves all the unique words in an output file ('vocab.txt'). We also store the word lengths in a dictionary, where key is the word length and value is the number of sentences having that length. We then plot a word length distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxsqxA_qI01-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unique_file(input_filename, output_filename):\n",
        "    input_file = open(input_filename, 'r')\n",
        "    file_contents = input_file.read()\n",
        "    input_file.close()\n",
        "    duplicates = set()\n",
        "    word_list = file_contents.split()\n",
        "    file = open(output_filename, 'w')\n",
        "    for word in word_list:\n",
        "        if word.lower() not in duplicates:\n",
        "            duplicates.add(word.lower())\n",
        "            file.write(str(word.lower()) + \"\\n\")\n",
        "    file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KHkFCQvS1b1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "wordcounts = defaultdict(int)\n",
        "d = dict()\n",
        "with open(root_folder+\"train.txt\") as f:\n",
        "    text = f.read()\n",
        "    sentences = text.split('.')\n",
        "    for sentence in sentences:\n",
        "        words = sentence.split(' ')\n",
        "        wordcounts[len(words)] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwSZYp5dYA5w",
        "colab_type": "code",
        "outputId": "282b288a-8d1b-46fe-cc73-a456b9d6f690",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "lists = sorted(wordcounts.items())[:50] # sorted by key, return a list of tuples\n",
        "\n",
        "x, y = zip(*lists) # unpack a list of pairs into two tuples\n",
        "plt.title(\"Distribution of word lengths\")\n",
        "plt.xlabel(\"No of words\")\n",
        "plt.ylabel(\"No of sentences\")\n",
        "plt.plot(x, y)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvqSI2g0KNwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unique_file(root_folder+\"train.txt\", root_folder+\"vocab.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1sGMnLikRd_",
        "colab_type": "text"
      },
      "source": [
        "'numerize_sequence' converts each word to its corresponding mapping to a number. 'pad_sequence' adds pad tokens to a sentence till  it is of the length 'to_length' (in this case, 20)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucgsqX8vkcih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def numerize_sequence(tokenized):\n",
        "    return [w2i.get(w, unkI) for w in tokenized]\n",
        "def pad_sequence(numerized, pad_index, to_length):\n",
        "    pad = numerized[:to_length]\n",
        "    padded = pad + [pad_index] * (to_length - len(pad))\n",
        "    mask = [w != pad_index for w in padded]\n",
        "    return padded, mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C8fYSvVpzMg",
        "colab_type": "text"
      },
      "source": [
        "I have manually added 3 tokens 'UNK', 'PAD', 'START' to vocab.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBHl7ZMKrm5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = root_folder + \"vocab.txt\"\n",
        "with open(file) as f:\n",
        "    vocabulary = f.read().split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1-P4bK5l2xt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2i = {w: i for i, w in enumerate(vocabulary)}\n",
        "input_length = 20\n",
        "unkI, padI, start_index = w2i['UNK'], w2i['PAD'], w2i['<START>']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq_ophWeu1ig",
        "colab_type": "text"
      },
      "source": [
        "Now, I break each sentence into the dataset into following 4 parts:\n",
        "- title - The actual text\n",
        "- numerized - Numerized version of the text\n",
        "- mask - Gives information about how long each sentence is\n",
        "- part - Whether the sentence is part of training or validation (every 10th sentence is put into validation set)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc9gW3o4hNgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = []\n",
        "i = 0\n",
        "input_length = 20\n",
        "with open(root_folder+\"train.txt\") as f:\n",
        "    text = f.read()\n",
        "    sentences = text.split('.')\n",
        "    for sentence in sentences:\n",
        "      data.append(dict())\n",
        "      sentence = sentence.replace('\\n', '')\n",
        "      words = sentence.split(' ')\n",
        "      l = min(len(words), input_length) \n",
        "      sentence = ''\n",
        "      for i in range(1, l): \n",
        "        sentence = sentence+words[i] + ' ' # LSTM is trained on lower-cased headlines\n",
        "      data[-1][\"title\"] = sentence\n",
        "\n",
        "      # Tokenize the sentence\n",
        "      tokenized = tokenizer.word_tokenizer(sentence)\n",
        "\n",
        "      # numerize the tokenized headline\n",
        "      numerized = numerize_sequence(tokenized)\n",
        "      padI = w2i['PAD']\n",
        "      numerized, mask = pad_sequence(numerized, padI, input_length) # Append appropriate PAD tokens\n",
        "\n",
        "      data[-1]['numerized'] = numerized\n",
        "\n",
        "      # apply mask\n",
        "      data[-1]['mask'] = mask\n",
        "\n",
        "      # split into train and test\n",
        "      if i%10:\n",
        "        data[-1]['part'] = \"train\"\n",
        "      else:\n",
        "        data[-1]['part'] = \"validation\"\n",
        "      i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88It-1d5mCsh",
        "colab_type": "code",
        "outputId": "beec1ec3-f7c9-4e43-c6c3-06690dc2d88b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "data[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mask': [True,\n",
              "  True,\n",
              "  True,\n",
              "  True,\n",
              "  True,\n",
              "  True,\n",
              "  True,\n",
              "  True,\n",
              "  True,\n",
              "  True,\n",
              "  True,\n",
              "  True,\n",
              "  True,\n",
              "  False,\n",
              "  False,\n",
              "  False,\n",
              "  False,\n",
              "  False,\n",
              "  False,\n",
              "  False],\n",
              " 'numerized': [39,\n",
              "  40,\n",
              "  41,\n",
              "  24,\n",
              "  7,\n",
              "  42,\n",
              "  12,\n",
              "  43,\n",
              "  18,\n",
              "  44,\n",
              "  45,\n",
              "  46,\n",
              "  21003,\n",
              "  2,\n",
              "  2,\n",
              "  2,\n",
              "  2,\n",
              "  2,\n",
              "  2,\n",
              "  2],\n",
              " 'part': 'train',\n",
              " 'title': 'never once occurred to me that the fumbling might be a mere mistake '}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZFSefKns6_-",
        "colab_type": "code",
        "outputId": "3c5ef413-2e67-44de-815e-153c5e08001c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "d_train = [d for d in data if d['part'] == 'train']\n",
        "d_valid = [d for d in data if d['part'] == 'validation']\n",
        "\n",
        "print(\"Number of training samples:\",len(d_train))\n",
        "print(\"Number of validation samples:\",len(d_valid))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples: 19512\n",
            "Number of validation samples: 564\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7KsaMTYZMUT",
        "colab_type": "text"
      },
      "source": [
        "## Making pipeline to feed batches of sentences into the neural net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UQrGDCKIZF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_batch(dataset, batch_size):\n",
        "\n",
        "    # choose random indices for each batch\n",
        "    indices = list(np.random.randint(0, len(dataset), size=batch_size))\n",
        "    \n",
        "    # Recover what the entries for the batch are\n",
        "    batch = [dataset[i] for i in indices]\n",
        "    \n",
        "    # Get numerized for this input, each element of the dataset has a 'numerized' key\n",
        "    batch_numerized = [i['numerized'] for i in batch]\n",
        "\n",
        "    # Create an array of start_indices that will be concatenated at beginning of input\n",
        "    start_tokens = np.array([start_index]*batch_size).reshape((batch_size, 1))\n",
        "\n",
        "    # Concatenate the start_tokens with the rest of the input\n",
        "    batch_input = np.concatenate((start_tokens, batch_numerized), axis=1) \n",
        "\n",
        "    # Remove the last word from each element in the batch\n",
        "    batch_input = batch_input[:,:-1]\n",
        "    #print(batch_input.shape)\n",
        "    \n",
        "    batch_target = batch_numerized\n",
        "\n",
        "    # The target-mask to know which places have 'PAD' token\n",
        "    batch_target_mask = np.array([a['mask'] for a in batch])\n",
        "       \n",
        "    return batch_input, batch_target, batch_target_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36SjiEj-ZMUe",
        "colab_type": "text"
      },
      "source": [
        "# RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH7PzTdLZMUj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "- I will use placeholders to feed in inputs and actual targets.\n",
        "- Then I will use an RNN to get an output, and project it on the dimension of size 'vocabulary' so that we get probability estimates for each word in the vocabulary.\n",
        "- The optimizer will try to minimize the loss between the predicted word and the actual word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK3UTjGhZMUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LanguageModel():\n",
        "    def __init__(self, input_length, vocab_size, rnn_size, learning_rate=1e-3):\n",
        "        \n",
        "        # Creating placeholders\n",
        "        self.input_num = tf.placeholder(tf.int32, shape=[None, input_length])\n",
        "        self.targets = tf.placeholder(tf.int32, shape=[None, input_length])\n",
        "        self.targets_mask = tf.placeholder(tf.int32, shape=[None, input_length])\n",
        "\n",
        "        # Mapping each word in vocab into a vector of rnn_size size.\n",
        "        embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size], dtype = tf.float32)\n",
        "        input_emb = tf.nn.embedding_lookup(embedding, self.input_num)\n",
        "        \n",
        "        # Create the RNN (I use LSTM with Dropout after trying some other combinations)   \n",
        "        lm_cell = tf.nn.rnn_cell.LSTMCell(rnn_size)\n",
        "        lm_cell = tf.nn.rnn_cell.DropoutWrapper(lm_cell, input_keep_prob = 0.8)\n",
        "        # dynamic_rnn introduces the recurrence\n",
        "        outputs, states = tf.nn.dynamic_rnn(lm_cell, input_emb, dtype = tf.float32)\n",
        "\n",
        "        # Project rnn output to vocab_size\n",
        "        self.output_logits = tf.layers.dense(outputs,units = vocab_size ) \n",
        "\n",
        "        # calculate loss\n",
        "        self.loss = tf.losses.sparse_softmax_cross_entropy(labels = self.targets, logits = self.output_logits, weights = self.targets_mask)                 \n",
        "\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)       \n",
        "\n",
        "        self.global_step = tf.train.get_or_create_global_step()\n",
        "        self.train_op = optimizer.minimize(self.loss, global_step = self.global_step)\n",
        "        self.saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jH3sPmhZMUu",
        "colab_type": "code",
        "outputId": "f057ee10-cf00-4136-a937-ff3942346f5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "tf.reset_default_graph()# clear variable space\n",
        "vocab_size = len(vocabulary)\n",
        "model = LanguageModel(input_length=input_length, vocab_size=vocab_size, rnn_size=256, learning_rate=1e-3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0627 07:26:46.582612 140158910969728 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0627 07:26:46.601772 140158910969728 deprecation.py:323] From <ipython-input-12-efb174729623>:14: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "W0627 07:26:46.609007 140158910969728 deprecation.py:323] From <ipython-input-12-efb174729623>:17: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "W0627 07:26:46.744477 140158910969728 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0627 07:26:47.465925 140158910969728 deprecation.py:323] From <ipython-input-12-efb174729623>:20: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0627 07:26:48.012015 140158910969728 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krUmQsMvZMUz",
        "colab_type": "text"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_4Zyh6SZMU6",
        "colab_type": "text"
      },
      "source": [
        "I experimented with a range of hyperparameters :\n",
        "- Learning rate\n",
        "- Type of optimizer ( Adam, SGD etc)\n",
        "- Using dropout \n",
        "- Architecture - Using rnn cell, LSTM cell.\n",
        "\n",
        "Finally I train my model for 10000 iterations with a batch size of 16, so roughly 8 epochs  (takes around 20 minutes on my system). \n",
        "Final parameters are:\n",
        "Learning rate of 1e-3, LSTM cell with size of 256 with keep_prob of 0.8, Adam Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhk4QFSs4Eu8",
        "colab_type": "code",
        "outputId": "624c9346-def4-48cd-d25d-8699efbd2793",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "experiment = root_folder+\"models/language_model_1\"\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for i in range(10000):\n",
        "      batch_size = 16\n",
        "      batch_input, batch_target, batch_target_mask = build_batch(d_train, batch_size)\n",
        "      feed = {model.input_num: batch_input, model.targets: batch_target, model.targets_mask: batch_target_mask}\n",
        "\n",
        "      step, train_loss, _ = sess.run([model.global_step, model.loss, model.train_op], feed_dict=feed)\n",
        "      if i%2000 == 0: \n",
        "        print(f\"Loss after {i} iterations: \", train_loss)\n",
        "    \n",
        "    model.saver.save(sess, experiment)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss after 0 iterations:  10.769726\n",
            "Loss after 2000 iterations:  5.676057\n",
            "Loss after 4000 iterations:  5.030911\n",
            "Loss after 6000 iterations:  5.150795\n",
            "Loss after 8000 iterations:  4.4185038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w8qCQJR_3Wa",
        "colab_type": "text"
      },
      "source": [
        "## Checking loss on validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDPSMuPMZMVT",
        "colab_type": "code",
        "outputId": "d70126c7-c38e-4c56-a03b-7390869fca06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "model_file = root_folder+\"models/language_model_1\"\n",
        "with tf.Session() as sess:\n",
        "    model.saver.restore(sess, model_file)\n",
        "    eval_input, eval_target, eval_target_mask = build_batch(d_valid, 500)\n",
        "    feed = {model.input_num: eval_input, model.targets: eval_target, model.targets_mask: eval_target_mask}\n",
        "    eval_loss = sess.run([model.loss], feed_dict=feed)\n",
        "    print(\"Evaluation set loss:\", eval_loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0627 07:54:23.348746 140158910969728 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation set loss: [6.521181]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDV6qwMhZMWQ",
        "colab_type": "text"
      },
      "source": [
        "## Text Generation\n",
        "\n",
        "For generating new text, I will enter the first few words. Then the trained model will keep on picking the next most likely word ( given by the argmax) and generate the sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gopFoDCTZMWT",
        "colab_type": "code",
        "outputId": "8e0c0432-edc6-44cc-f2d6-0f255811af23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    model.saver.restore(sess, model_file)\n",
        "    headline_starters = [\"i even \", \"all that\", \"where was\", \"sometimes it\"]\n",
        "    input_length = 20\n",
        "    for headline_starter in headline_starters:\n",
        "        print(\"===================\")\n",
        "        print(\"Generating headline starting with: \"+headline_starter)\n",
        "\n",
        "        tokenized = tokenizer.word_tokenizer(headline_starter)\n",
        "        current_build = [start_index] + numerize_sequence(tokenized)\n",
        "\n",
        "        while len(current_build) < input_length:\n",
        "            # Pad so that sentence can be fed in the rnn\n",
        "            current_padded = current_build[:input_length] + [padI] * (input_length - len(current_build))\n",
        "            current_padded = np.array([current_padded])\n",
        "\n",
        "            # Obtain the logits for the current padded sequence\n",
        "            feed = {model.input_num: current_padded}\n",
        "            logits = np.array(sess.run(model.output_logits, feed_dict=feed))\n",
        "            \n",
        "            # Get logits for the last non-pad inputs\n",
        "            last_logits = logits[0, len(current_build), :]\n",
        "            \n",
        "            # Find the highest scoring word in the last_logits, excluding punctuation signs for now as the model hasnt yet learnt how to incorporate them\n",
        "            word_idx = np.argsort(last_logits)[-10:]\n",
        "            words = [vocabulary[w] for w in word_idx]\n",
        "            for idx in word_idx[::-1]:\n",
        "              if vocabulary[idx] not in [\"UNK\", \"PAD\", \"<START>\", \"'\", \"-\",\"(\", \")\", \",\", \"?\"]: \n",
        "                current_build.append(idx)\n",
        "                break\n",
        "        \n",
        "        # use vocabulary to map numbers back to words\n",
        "        produced_sentence = [vocabulary[idx] for idx in current_build]\n",
        "        print(' '.join(produced_sentence))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===================\n",
            "Generating headline starting with: i even \n",
            "<START> i even were in and a stampede which the of birds in and a fought in a dedicated in\n",
            "===================\n",
            "Generating headline starting with: all that\n",
            "<START> all that have the of Harris in the of causes in the of a themselves in a of black\n",
            "===================\n",
            "Generating headline starting with: where was\n",
            "<START> where was in a of and a time a time in a of effort to the of causes in\n",
            "===================\n",
            "Generating headline starting with: sometimes it\n",
            "<START> sometimes it that the of folks in the of and a packet in the shift in the of and\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oYS2JVeSVJ8",
        "colab_type": "text"
      },
      "source": [
        "## Using pretrained network\n",
        "\n",
        "The model above serves just to demonstrate the working of a simple rnn, it requires substantial work in terms of making a deeper architecture and training for more time to achieve better performance. For demonstration purposes, I will now use a pretrained char-based rnn (https://github.com/minimaxir/textgenrnn) to show how text is generated after training for 3 epochs. Being a char-based rnn, the model makes some spelling mistakes while generating words, but it does a better job at generating longer sentences than our model. Similarly, one can take other pretrained networks like GPT-2 and use them to generate sentences for our task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-im-L_xYGRrX",
        "colab_type": "code",
        "outputId": "d9620db6-a693-4908-fc30-65de2f267e7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from textgenrnn import textgenrnn\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpo2P6kzSk70",
        "colab_type": "code",
        "outputId": "37fdae44-edcc-41ca-a2df-3ed16ebb773a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "root_folder = \"/content/gdrive/My Drive/text_generation/\"\n",
        "textgen.train_from_file(root_folder + 'train.txt', num_epochs=3)\n",
        "textgen.generate()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19,578 texts collected.\n",
            "Training on 2,937,742 character sequences.\n",
            "Epoch 1/3\n",
            "22951/22951 [==============================] - 346s 15ms/step - loss: 1.4213\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "I said the secret of the secret of the steps of the shoulder of the countenance and the continual secret of the contagion of the secret of the sun and the character of the start deserves of the shoulder and secret of the soul of the short best the short and contrary of the countenance of the short\n",
            "\n",
            "I said the press of the streets of the life of the shoulder of the contemple deserted me to the most conscious of the subject of the sound of the secret of the shoulder of the contribuation of the shoulder and the soul of the deserted contrary of the deserted matter and chief and contribuation of \n",
            "\n",
            "I say the shoulder of the shoulder of the careful countenance of the streets of the secret of the continual preserves of the being and contagion of the great and descendation of the opinous of the countenance of the short and press of the contemplated means of the surranging the shoulder of the se\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "It is by the same passion of the short, and the left ship of the contementual creature Varbay who had been an excited soul of the books, the roots of whom I could be death to star the days of my inclesses or my proper countenance.\n",
            "\n",
            "I shall make the mirror down the performers of the souls and food and press the the present tooks and life of the gentle breaker who did not afford that the chief shoulder and pressure in the stease of the conscious things are sometimes and as the Cerin of the world.\n",
            "\n",
            "And it was dark and distinct to my counter and his perfumbers of the star in the sufficient days of the creature open the minute careful little filling and continual meaning and character of the secret of every desight, and the secret of the creature and became on conscious point of the part of th\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "Not not a sreim, they entered to prevent the razific of veagure, and plendant, because he harmmocks should the for himself.\n",
            "\n",
            "He continise usually disturbed, nor prey's Lamma, had been livernier over her dwell.\n",
            "\n",
            "I may be milmaties on enough,\" he were entirely before for less.\n",
            "\n",
            "Epoch 2/3\n",
            "22951/22951 [==============================] - 340s 15ms/step - loss: 1.3524\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "I was a company of the consequence of the strange companions of the strange constanctions of the strange strange constantion of the consciousness of the strange strange death of the streets of the deceive that he had not the sea of the strange southward had been a considerable constant of the stra\n",
            "\n",
            "I have not the strange sense of the strange constant and the strange past and an earth of the strange strange state of the commonsions of the strange reason and communizations and the strange conscious of the stars of the strange consequence of the consideration of the water and strange conscious \n",
            "\n",
            "The strange and this state of the strange strange described to the constant of the strange strange conscious of the consideration of the contradity of the strange and strange consciousness and the strange spectators of the strange and second and strange dissolution of the strange conscious was a p\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "Amazerations were low, and combined to the stars and realms of the wisdom in a distance.\n",
            "\n",
            "So fire, the districation of the thing shall feel the silent through his under the air.\n",
            "\n",
            "I earth my natural signs of the streets of the side of the ancient second that arranged and dead and common of the conded from the courted and abode, at least content the new appearance.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "The life on thitient was balance a full idea of my aethers, thus: read, puts will vass.\n",
            "\n",
            "He saw the less consequence.\n",
            "\n",
            "Mrs dark hilar and evidence of the evil or to the coat more at  Usonn, sardocities by Jypos left the goodish and police if the door lap could sudden togetherny above in the Snay, and that I but diffisted well to earth a habitate dearystime bent too, dearly the majely are others home, Rajea, I nece\n",
            "\n",
            "Epoch 3/3\n",
            "22951/22951 [==============================] - 335s 15ms/step - loss: 1.2906\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "The body of the strange state of the strange part of the following of the wild death of the world of the whole and the strange and the properties of the strange state of the fact of the arms of the consciousness of the state of the state of the strange companions of the steps of the accustomed thi\n",
            "\n",
            "The strange dead and the strange descent of the part of the street of the beauty of the part of the man of the stars of the balloon of the man of the strange day of the moon of the man of the stars of the strange and the state of the sun of the town of the dead part of the water of the strange dea\n",
            "\n",
            "I was a strange day of the dead and the body of the body of the dead and the proportion of the ancient described the moon of the dead and the life of the considerable streets of the morning of the man of the man of the street and the sharp of the shadow of the dark of the man of the strange partic\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "I sit to the comparation of the warmed put of the beast of the interest, as in the depth and I am so fancy to the body of the suspicion and making it as the old crowd which is not the way displayed a resignation of his present singular part of part of the impulse of the old man had seen the most b\n",
            "\n",
            "In the table of the sense of the shadow of the story and some position of the subject of the world to possible the sea of the stream of increased, the wild three and all the personal door and all the fashionable man from her father that is the shore of the body of a capacity of a change, and and t\n",
            "\n",
            "The stairs of the limbs which we were heard to the also the dead and the straight which the ancient summonts of the side of the returned shadow was not to the observations of the side of a black hour and a very death and the world of a night by its more and completely need of the morning of the or\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "I thus placed me that it is accompanied it al, we must not reven probable fiend.\n",
            "\n",
            "These ones had made me were overstood of suspicious air, relief as the Greek, apartless, which before a thing, and a dullen; and, that you undid the big first perceived, balm at one sound younger into this brain and we had traced every bottom in my skyle, and the attention altogether should become\n",
            "\n",
            "The way of a faculty faced a mists of impossible storming determints of night, century he will have been combinatives in silence.\n",
            "\n",
            "The monster was the struggles of a possessive character of a stair in the darkness of the interest of the screaming best and the agony of the assertion of the chimney of my deeper which had been a best of a reach of a black despaired with my little buildings of that fear of the memory.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc9O1QHYTopU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}